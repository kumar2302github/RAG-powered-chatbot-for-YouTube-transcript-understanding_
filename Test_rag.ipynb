{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa403480",
   "metadata": {},
   "outputs": [],
   "source": [
    "#library for vectore store,remove duplicates cosine simalrity and clustering\n",
    "import os  #Create/delete folders like vector_store/, handle file paths\n",
    "import logging\n",
    "import numpy as np\n",
    "import hdbscan    #Group similar chunks (clustering)\n",
    "from typing import List, Tuple, Any, Optional\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics.pairwise import cosine_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47a7216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain.vectorstores.utils import maximal_marginal_relevance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a90193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e38d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c542e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_DB_DIR = \"vector_store\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "LLM_MODEL = \"gemini-1.5-flash\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67780ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "    logging.info(\"Embedding model loaded successfully\")\n",
    "except Exception as e:\n",
    "    logging.error(\"Failed to load embedding model\", exc_info=True)\n",
    "    embedding_model = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1ec041",
   "metadata": {},
   "source": [
    "Gemini Config + Chunking + Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4392e438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_gemini_api(api_key: str) -> bool:\n",
    "    if not api_key:\n",
    "        logging.error(\"No API key provided\")\n",
    "        return False\n",
    "    try:\n",
    "        genai.configure(api_key=api_key)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        # Print the full error message to the terminal\n",
    "        logging.error(\"Gemini API setup failed\", exc_info=True)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e46406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the big transcript into many small chunks\n",
    "def split_transcript(text: str, chunk_size: int = 500, chunk_overlap: int = 0) -> List[str]:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "                         chunk_size=chunk_size,\n",
    "                        chunk_overlap=chunk_overlap)\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb3a555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts text to lowercase, trims edges, and removes extra spaces between words\n",
    "def normalize(text: str) -> str:\n",
    "    return \" \".join(text.lower().strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75777fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes duplicate chunks based on cleaned normalized text, while keeping original order\n",
    "\n",
    "def remove_duplicates(chunks):\n",
    "    unique = OrderedDict()\n",
    "    for chunk in chunks:\n",
    "        key = normalize(chunk)\n",
    "        if key not in unique:\n",
    "            unique[key] = chunk\n",
    "    return list(unique.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a31ac7",
   "metadata": {},
   "source": [
    "Vector DB Management (Save / Load / Initialize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ffa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad424226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_vector_db(video_id):\n",
    "    # Create the full path to the vector DB folder for the video\n",
    "    folder_path = os.path.join(VECTOR_DB_DIR, video_id)\n",
    "\n",
    "    if os.path.exists(folder_path):\n",
    "        try:\n",
    "            # Delete the folder and everything inside it\n",
    "            shutil.rmtree(folder_path)\n",
    "\n",
    "        except OSError as error:\n",
    "            logging.error(\"Failed to remove vector database: %s\" % error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8330e317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_vector_db(video_id: str) -> None:\n",
    "    reset_vector_db(video_id)\n",
    "    pkl_path = os.path.join(VECTOR_DB_DIR, f\"{video_id}.pkl\")\n",
    "    if os.path.exists(pkl_path):\n",
    "        try:\n",
    "            os.remove(pkl_path)\n",
    "        except OSError as e:\n",
    "            logging.error(f\"Failed to remove .pkl file: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b7c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_vector_dbs():\n",
    "    # If main vector store doesn't exist, return an empty list\n",
    "    if not os.path.exists(VECTOR_DB_DIR):\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        # Get a list of all folder names for showing in the UI\n",
    "        folder_list = []\n",
    "        for name in os.listdir(VECTOR_DB_DIR):\n",
    "            full_path = os.path.join(VECTOR_DB_DIR, name)\n",
    "            if os.path.isdir(full_path):\n",
    "                folder_list.append(name)\n",
    "        return folder_list\n",
    "\n",
    "    except OSError as error:\n",
    "        logging.error(\"Error while listing vector DBs: %s\" % error)\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3748a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_rag(transcript_text: str, video_id: str) -> Optional[Chroma]:\n",
    "    #stop if embedding model is not loaded\n",
    "    if embedding_model is None:\n",
    "        return None\n",
    "\n",
    "    #folder path where the vector DB will be stored \n",
    "    persist_path = os.path.join(VECTOR_DB_DIR, video_id)\n",
    "\n",
    "    # if vectordb exists already\n",
    "    os.makedirs(VECTOR_DB_DIR, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Checking if this vector DB already exists in the folder\n",
    "        if os.path.exists(os.path.join(persist_path, \"chroma-collections.parquet\")):\n",
    "            try:\n",
    "                vectordb = Chroma(persist_directory=persist_path, embedding_function=embedding_model)\n",
    "                _ = vectordb.get(limit=1)\n",
    "                return vectordb\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"DB load failed, rebuilding: {e}\")\n",
    "                reset_vector_db(video_id)\n",
    "        \n",
    "        #if no vector DB exists, creating a new one\n",
    "        chunks = split_transcript(transcript_text)\n",
    "        chunks = remove_duplicates(chunks)\n",
    "        if not chunks:\n",
    "            return None\n",
    "\n",
    "        #Wrap each chunk in a Document object\n",
    "        docs = []\n",
    "        for chunk in chunks:\n",
    "            docs.append(Document(page_content=chunk))\n",
    "\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=docs,\n",
    "            embedding=embedding_model,\n",
    "            persist_directory=persist_path\n",
    "        )\n",
    "        # Saving to disk\n",
    "        vectordb.persist()\n",
    "        return vectordb\n",
    "\n",
    "        \n",
    "    #for errors \n",
    "    except Exception as e:\n",
    "        logging.error(f\"RAG init failed: {e}\", exc_info=True)\n",
    "        reset_vector_db(video_id)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11907f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_embeddings(docs, embeddings):\n",
    "\n",
    "    valid_docs = []\n",
    "    valid_embeddings = []\n",
    "\n",
    "    # Make sure both lists are the same length to avoid mismatch errors\n",
    "    min_len = min(len(docs), len(embeddings))\n",
    "\n",
    "    # Looping through both docs and embeddings safely\n",
    "    for i in range(min_len):\n",
    "        doc = docs[i]\n",
    "        emb = embeddings[i]\n",
    "\n",
    "        # Checking if the embedding is a list of numbers(floats or ints)\n",
    "        if isinstance(emb, list) and all(isinstance(x, (float, int)) for x in emb):\n",
    "            # Keeping only the valid ones\n",
    "            valid_docs.append(doc)\n",
    "            valid_embeddings.append(np.array(emb, dtype=np.float32))\n",
    "\n",
    "    # Return the filtered list of valid docs and embeddings\n",
    "    return valid_docs, valid_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c2b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_and_select_indices(docs, embeddings, min_cluster_size=2):\n",
    "    # Filter out any bad embeddings\n",
    "    valid_docs, valid_embeddings = _validate_embeddings(docs, embeddings)\n",
    "\n",
    "    # If no usable embeddings, just return the first few docs\n",
    "    if not valid_embeddings:\n",
    "        return list(range(min(len(docs), 5)))\n",
    "\n",
    "    # If too few for clustering, skip it and return all valid ones\n",
    "    if len(valid_embeddings) < min_cluster_size:\n",
    "        return list(range(len(valid_docs)))\n",
    "\n",
    "    try:\n",
    "        # Combine all embeddings into one big matrix\n",
    "        X = np.vstack(valid_embeddings)\n",
    "\n",
    "        # Calculate distances between every pair of embeddings\n",
    "        distance_matrix = cosine_distances(X)\n",
    "\n",
    "        # Set the diagonal (self-to-self) distances to 0\n",
    "        np.fill_diagonal(distance_matrix, 0)\n",
    "\n",
    "        # clustering using HDBSCAN on the distance matrix\n",
    "        clusterer = hdbscan.HDBSCAN(\n",
    "            metric=\"precomputed\",\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            allow_single_cluster=True\n",
    "        )\n",
    "        labels = clusterer.fit_predict(distance_matrix.astype(np.float64))\n",
    "\n",
    "        # Group documents by cluster label\n",
    "        clusters = {}\n",
    "        noise_count = 0\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "\n",
    "            # Give noise points their own unique label\n",
    "            if label == -1:\n",
    "                cluster_label = \"noise-\" + str(noise_count)\n",
    "                noise_count += 1\n",
    "            else:\n",
    "                cluster_label = label\n",
    "\n",
    "            if cluster_label not in clusters:\n",
    "                clusters[cluster_label] = []\n",
    "\n",
    "            clusters[cluster_label].append(i)\n",
    "\n",
    "        # From each cluster, pick the first document\n",
    "        selected_indices = []\n",
    "        for cluster_id in sorted(clusters.keys()):\n",
    "            first_doc_index = clusters[cluster_id][0]\n",
    "            selected_indices.append(first_doc_index)\n",
    "\n",
    "        return selected_indices\n",
    "\n",
    "    except Exception as error:\n",
    "        # If clustering fails for any reason, return first few valid ones\n",
    "        logging.error(\"Clustering failed: %s\" % error)\n",
    "        return list(range(min(len(valid_docs), 5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e7dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_retrieve(vectordb, query, top_k=5, score_threshold=0.4, lambda_mult=0.7, max_tokens=1000):\n",
    "    # Make sure the vector DB and embedding model are ready\n",
    "    if embedding_model is None or not isinstance(vectordb, Chroma):\n",
    "        return [], []\n",
    "\n",
    "    try:\n",
    "        # Step 1: Convert the user query into an embedding vector\n",
    "        query_embedding = embedding_model.embed_query(query)\n",
    "\n",
    "        # Step 2: Get initial results from the vector DB having more than k chunks\n",
    "        results = vectordb.similarity_search_with_relevance_scores(query, k=max(top_k * 5, 20))\n",
    "\n",
    "        # Step 3: Filter out bad matches (keep only results with low score)\n",
    "        filtered_results = []\n",
    "        for doc, score in results:\n",
    "            if score <= score_threshold:\n",
    "                filtered_results.append((doc, score))\n",
    "\n",
    "        # If nothing passed the threshold, stop\n",
    "        if not filtered_results:\n",
    "            return [], []\n",
    "\n",
    "        # Step 4: Separate docs, scores, and re-embed the texts\n",
    "        filtered_docs = []\n",
    "        filtered_scores = []\n",
    "        for doc, score in filtered_results:\n",
    "            filtered_docs.append(doc)\n",
    "            filtered_scores.append(score)\n",
    "\n",
    "        # Embed all filtered docs again\n",
    "        texts_to_embed = [doc.page_content for doc in filtered_docs]\n",
    "        filtered_embeddings = embedding_model.embed_documents(texts_to_embed)\n",
    "\n",
    "        # Step 5: Cluster and select representatives\n",
    "        selected_indices = cluster_and_select_indices(filtered_docs, filtered_embeddings)\n",
    "\n",
    "        unique_docs = [filtered_docs[i] for i in selected_indices]\n",
    "        unique_embeddings = [filtered_embeddings[i] for i in selected_indices]\n",
    "        unique_scores = [filtered_scores[i] for i in selected_indices]\n",
    "\n",
    "        # Step 6: Apply Maximal Marginal Relevance to select top diverse and relevant chunks\n",
    "        mmr_embeddings = [np.array(embedding, dtype=np.float32) for embedding in unique_embeddings]\n",
    "        mmr_indices = maximal_marginal_relevance(\n",
    "            np.array(query_embedding, dtype=np.float32),\n",
    "            mmr_embeddings,\n",
    "            lambda_mult=lambda_mult,\n",
    "            k=min(top_k, len(unique_docs))\n",
    "        )\n",
    "\n",
    "        # Get the selected documents and scores based on MMR\n",
    "        selected_docs = [unique_docs[i] for i in mmr_indices]\n",
    "        selected_scores = [unique_scores[i] for i in mmr_indices]\n",
    "\n",
    "        # Step 7: Keep adding chunks until the token limit is hit\n",
    "        final_chunks = []\n",
    "        final_scores = []\n",
    "        total_tokens = 0\n",
    "\n",
    "        for doc, score in zip(selected_docs, selected_scores):\n",
    "            tokens_in_doc = len(doc.page_content.split())\n",
    "\n",
    "            if total_tokens + tokens_in_doc <= max_tokens:\n",
    "                final_chunks.append(doc.page_content)\n",
    "                final_scores.append(score)\n",
    "                total_tokens += tokens_in_doc\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return final_chunks, final_scores\n",
    "\n",
    "    except Exception as error:\n",
    "        # Catch and log any unexpected error\n",
    "        logging.error(\"Retrieval failed: %s\" % error)\n",
    "        return [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e209d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(relevant_texts: List[str], user_query: str) -> str:\n",
    "    if not relevant_texts:\n",
    "        return \"No relevant content found. Try a different question.\"\n",
    "\n",
    "    context = \"\\n\\n---\\n\\n\".join(relevant_texts)\n",
    "    prompt = f\"\"\"\n",
    "You are Carl Saganâ€”poetic, insightful, and curious.\n",
    "Use real-world analogies and clear, thoughtful language.\n",
    "Transcript Context:\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "User Question:\n",
    "{user_query}\n",
    "\n",
    "Your Answer (as Carl Sagan, using only the transcript context, if not in there say \"I don't know\"):\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel(LLM_MODEL)\n",
    "        response = model.generate_content(prompt)\n",
    "\n",
    "        if hasattr(response, 'text'):\n",
    "            return response.text\n",
    "        elif hasattr(response, 'prompt_feedback'):\n",
    "            return f\"Content blocked: {response.prompt_feedback.block_reason}\"\n",
    "        else:\n",
    "            return \"Failed to generate response\"\n",
    "    except Exception as e:\n",
    "        if \"API_KEY\" in str(e):\n",
    "            return \"API key error - check your configuration\"\n",
    "        return f\"Error generating response: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce5c6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed53e76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
